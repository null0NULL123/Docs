{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f67d54",
   "metadata": {},
   "source": [
    "## 聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5809a95",
   "metadata": {},
   "source": [
    "\n",
    "无监督分类通常被称为“聚类”（Clustering）。它的目标是在没有预先标记类别的情况下，根据数据点之间的相似性将它们划分为不同的组（簇）。\n",
    "\n",
    "以下是一些常见的无监督聚类算法及其原理：\n",
    "\n",
    "1.  **K-Means (K-均值聚类)**\n",
    "    *   **原理**: K-Means是最经典的聚类算法之一。它试图将数据集划分为预先指定的 K 个簇。算法的目标是最小化每个数据点到其所属簇的质心（均值点）的距离平方和（即簇内方差）。\n",
    "    *   **步骤**:\n",
    "        1.  随机选择 K 个数据点作为初始的簇质心。\n",
    "        2.  **分配**: 将每个数据点分配给距离它最近的质心所代表的簇。\n",
    "        3.  **更新**: 重新计算每个簇的质心（通常是簇内所有点的均值）。\n",
    "        4.  重复步骤 2 和 3，直到簇的分配不再改变，或者质心位置变化很小，或者达到最大迭代次数。\n",
    "    *   **特点**:\n",
    "        *   简单、快速，对大数据集有效。\n",
    "        *   需要预先指定簇的数量 K。\n",
    "        *   对初始质心的选择敏感。\n",
    "        *   倾向于发现球状、大小相似的簇。\n",
    "        *   对异常值敏感。\n",
    "\n",
    "2.  **Hierarchical Clustering (层次聚类)**\n",
    "    *   **原理**: 层次聚类不是预设簇的数量，而是构建一个簇的层级结构（树状图，称为 Dendrogram）。有两种主要方式：\n",
    "        *   **凝聚型 (Agglomerative)**: 自底向上。开始时每个数据点自成一簇，然后迭代地合并最相似（距离最近）的两个簇，直到所有点合并成一个簇。\n",
    "        *   **分裂型 (Divisive)**: 自顶向下。开始时所有数据点在一个簇，然后迭代地将最不相似的簇分裂，直到每个点自成一簇或满足某个停止条件。\n",
    "    *   **关键**: 如何定义簇之间的距离（相似度），常见的有：\n",
    "        *   **单链接 (Single Linkage)**: 簇间最短距离。\n",
    "        *   **全链接 (Complete Linkage)**: 簇间最长距离。\n",
    "        *   **平均链接 (Average Linkage)**: 簇间所有点对距离的平均值。\n",
    "        *   **Ward 方法**: 最小化合并后簇内方差的增量。\n",
    "    *   **特点**:\n",
    "        *   不需要预先指定簇数 K，可以根据树状图决定。\n",
    "        *   可以揭示数据的层级结构。\n",
    "        *   计算复杂度通常高于 K-Means。\n",
    "        *   凝聚型方法一旦合并就无法撤销。\n",
    "\n",
    "3.  **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "    *   **原理**: 基于密度的聚类方法。它将紧密相连的区域划分为簇，并能识别出噪声点（异常值）。核心思想是：一个簇是由足够高密度的区域组成的。\n",
    "    *   **核心概念**:\n",
    "        *   **核心点 (Core Point)**: 在半径 `ε` (Epsilon) 内至少包含 `MinPts` 个邻居点的点。\n",
    "        *   **边界点 (Border Point)**: 不是核心点，但在某个核心点的 `ε` 邻域内。\n",
    "        *   **噪声点 (Noise Point)**: 既不是核心点也不是边界点。\n",
    "    *   **步骤**:\n",
    "        1.  从任意未访问点开始。\n",
    "        2.  如果该点是核心点，则创建一个新簇，并将所有从它密度可达（直接或间接通过其他核心点连接）的点加入该簇。\n",
    "        3.  如果该点是边界点或噪声点，暂时标记为噪声（可能稍后被发现是某个簇的边界点）。\n",
    "        4.  重复直到所有点都被访问。\n",
    "    *   **特点**:\n",
    "        *   能发现任意形状的簇。\n",
    "        *   对噪声不敏感。\n",
    "        *   不需要预先指定簇数 K。\n",
    "        *   结果对参数 `ε` 和 `MinPts` 的选择敏感。\n",
    "        *   对于密度变化很大的数据集效果可能不佳。\n",
    "\n",
    "4.  **Gaussian Mixture Models (GMM - 高斯混合模型)**\n",
    "    *   **原理**: GMM 是一种概率模型，它假设数据是由 K 个不同的高斯（正态）分布混合生成的。每个高斯分布代表一个簇。算法的目标是估计每个高斯分布的参数（均值、协方差）以及每个数据点属于各个高斯分布的概率。\n",
    "    *   **算法**: 通常使用期望最大化（Expectation-Maximization, EM）算法进行参数估计：\n",
    "        *   **E 步 (Expectation)**: 估计每个数据点由每个高斯分布生成的后验概率（“软分配”）。\n",
    "        *   **M 步 (Maximization)**: 根据 E 步得到的概率，重新估计每个高斯分布的参数（均值、协方差、混合系数）。\n",
    "        *   重复 E 步和 M 步直到收敛。\n",
    "    *   **特点**:\n",
    "        *   提供每个点属于各簇的概率（软聚类），而非硬性分配。\n",
    "        *   能适应椭球形状的簇（因为考虑了协方差）。\n",
    "        *   需要预先指定高斯分布的数量 K。\n",
    "        *   计算可能比 K-Means 复杂。\n",
    "        *   假设数据符合高斯分布。\n",
    "\n",
    "5.  **Affinity Propagation (近邻传播)**\n",
    "    *   **原理**: AP 算法不需指定簇的数量。它通过在数据点之间传递“信息”来工作。每个点都被视为潜在的“代表点”（Exemplar），算法通过迭代更新两种信息来确定最佳的代表点及其所属成员：\n",
    "        *   **责任 (Responsibility)**: 从点 i 发送到候选代表点 k 的信息，表示 k 适合作为 i 的代表点的证据。\n",
    "        *   **可用性 (Availability)**: 从候选代表点 k 发送到点 i 的信息，表示 i 选择 k 作为其代表点的合适程度。\n",
    "    *   **步骤**: 迭代更新这两种信息，直到收敛。最终，某个点的“自可用性”高，则它成为代表点。\n",
    "    *   **特点**:\n",
    "        *   不需要预先指定簇数 K。\n",
    "        *   对初始值不敏感（相比 K-Means）。\n",
    "        *   对输入参数“偏好（preference）”敏感，该参数影响发现的簇的数量。\n",
    "        *   计算复杂度较高，不适合非常大的数据集。\n",
    "\n",
    "这些是比较常见的几种无监督聚类算法。选择哪种算法取决于数据的特性、对簇形状的假设、是否知道簇的数量以及计算资源的限制等因素。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00f0aa",
   "metadata": {},
   "source": [
    "## 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a7cb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0168b353",
   "metadata": {},
   "source": [
    "\n",
    "降维是机器学习和数据分析中非常重要的一类技术，主要目的是在尽可能保留原始数据信息的前提下，减少数据的特征数量。以下是一些常用的降维算法及其原理：\n",
    "\n",
    "1.  **主成分分析 (Principal Component Analysis, PCA)**\n",
    "    *   **原理:** PCA 是一种最常用的线性降维方法。它的核心思想是找到数据中方差最大的方向（即主成分），并将数据投影到这些方向上。它通过对原始数据的协方差矩阵进行特征值分解（或奇异值分解 SVD），找到特征值最大的 K 个特征向量，这些特征向量构成了新的低维空间。数据点在新空间中的坐标就是降维后的表示。\n",
    "    *   **目标:** 最大化投影后数据的方差，使得数据在新的维度上尽可能分散，从而保留最多的原始信息。\n",
    "    *   **特点:** 无监督学习算法，易于实现，计算效率较高。对数据缩放敏感，假设数据的主要信息体现在方差较大的方向上。\n",
    "\n",
    "2.  **线性判别分析 (Linear Discriminant Analysis, LDA)**\n",
    "    *   **原理:** LDA 是一种**监督**的线性降维方法，通常用于分类任务的预处理。与 PCA 不同，LDA 的目标是找到一个低维空间，使得**不同类别**的数据点在该空间中尽可能分开，而**相同类别**的数据点尽可能聚集。它通过最大化类间散度（between-class scatter）与类内散度（within-class scatter）的比值来实现。\n",
    "    *   **目标:** 最大化类间距离，最小化类内距离。\n",
    "    *   **特点:** 监督学习算法，需要类别标签。降维后的维度最多为 `类别数 - 1`。对于非高斯分布的数据效果可能不佳。\n",
    "\n",
    "3. **特征嵌入 (Feature Embedding)**\n",
    "    *   **原理:** 特征嵌入是一种将高维稀疏特征映射到低维稠密空间的技术。它通常用于处理类别特征或文本数据。嵌入可以通过神经网络训练得到，或者使用预训练的嵌入模型（如 Word2Vec）。\n",
    "    *   **目标:** 捕捉特征之间的语义关系或相似性。\n",
    "    *   **特点:** 嵌入向量通常比原始特征表示更小且更稠密，能够捕捉到更多的语义信息。常用于自然语言处理和推荐系统等领域。\n",
    "\n",
    "4.  **统一流形逼近与投影 (Uniform Manifold Approximation and Projection, UMAP)**\n",
    "    *   **原理:** UMAP 是另一种现代的非线性降维技术，与 t-SNE 类似，也常用于可视化。它基于流形学习和拓扑数据分析的理论。UMAP 首先在高维空间中构建一个图（基于邻近点），表示数据的拓扑结构。然后，它尝试在低维空间中找到一个类似的图结构，通过优化使得两个图的结构尽可能一致。\n",
    "    *   **目标:** 在保持局部连接性的同时，尽可能保留数据的全局结构。\n",
    "    *   **特点:** 非线性，通常比 t-SNE 计算速度快，并且在保留数据全局结构方面可能表现更好。也常用于可视化。\n",
    "\n",
    "5.  **自编码器 (Autoencoders, AE)**\n",
    "    *   **原理:** 自编码器是一种基于神经网络的无监督降维方法。它由两部分组成：编码器（Encoder）和解码器（Decoder）。编码器将高维输入数据压缩到一个低维的潜在表示（latent representation，也称为瓶颈层 bottleneck）。解码器则尝试从这个低维表示中重建原始输入数据。通过训练神经网络最小化重建误差（即输入与输出之间的差异），编码器就学会了如何提取数据的主要特征并将其压缩到低维潜在空间中。这个潜在表示就是降维后的结果。\n",
    "    *   **目标:** 学习数据的有效压缩表示，最小化重建误差。\n",
    "    *   **特点:** 非线性（取决于网络结构），可以学习复杂的数据模式。需要设计和训练神经网络，计算成本可能较高。变种很多，如变分自编码器（VAE）、稀疏自编码器等。\n",
    "\n",
    "**总结:**\n",
    "\n",
    "*   **线性 vs. 非线性:** PCA 和 LDA 是线性方法，适用于数据结构相对简单的情况。t-SNE, UMAP, Autoencoders 是非线性方法，能处理更复杂的数据结构。\n",
    "*   **监督 vs. 无监督:** LDA 是监督方法，需要类别标签。PCA, t-SNE, UMAP, Autoencoders 是无监督方法。\n",
    "*   **应用场景:** PCA 常用于数据预处理、去噪。LDA 常用于分类任务的特征提取。t-SNE 和 UMAP 主要用于数据可视化和探索。Autoencoders 可用于特征提取、数据生成等。\n",
    "\n",
    "选择哪种降维算法取决于具体的任务目标、数据特性以及计算资源的限制。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6b522",
   "metadata": {},
   "source": [
    "\n",
    "我们来解释一下核方法（Kernel Methods）的原理和主要应用场景。\n",
    "\n",
    "**核方法的原理**\n",
    "\n",
    "核方法的核心思想是**将数据从原始的输入空间映射到一个更高维甚至无限维的特征空间（希尔伯特空间），使得原本在低维空间中线性不可分的数据，在高维空间中变得线性可分（或更容易处理）**。\n",
    "\n",
    "这个过程的关键在于**核技巧（Kernel Trick）**。很多机器学习算法（例如 SVM、PCA）的计算最终都可以归结为计算数据点之间的**内积（dot product）**。如果我们直接进行高维映射 \\(\\phi(x)\\)，计算 \\(\\langle \\phi(x_i), \\phi(x_j) \\rangle\\) 会非常复杂，甚至维度过高导致无法计算（维度灾难）。\n",
    "\n",
    "核技巧的神奇之处在于，它定义了一个**核函数 \\(K(x_i, x_j)\\)**，这个函数可以直接计算原始空间中两个点 \\(x_i\\) 和 \\(x_j\\) 在映射后的高维空间中的内积，即：\n",
    "\\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle \\]\n",
    "这意味着我们**不需要显式地计算高维映射 \\(\\phi(x)\\) 的具体形式**，也**不需要知道高维空间的具体维度**，就可以直接利用核函数得到高维空间中的内积结果。这样就巧妙地避免了高维计算的复杂性。\n",
    "\n",
    "常见的核函数有：\n",
    "\n",
    "*   **线性核（Linear Kernel）**: \\(K(x_i, x_j) = x_i^T x_j\\)。这相当于没有进行映射，适用于本身就线性可分的数据。\n",
    "*   **多项式核（Polynomial Kernel）**: \\(K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d\\)。可以将数据映射到 \\(d\\) 次多项式特征空间。\n",
    "*   **高斯核（Gaussian Kernel / RBF Kernel）**: \\(K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)\\)。这是最常用的一种核函数，可以将数据映射到无限维空间，能够处理非常复杂的非线性关系。\n",
    "*   **Sigmoid 核（Sigmoid Kernel）**: \\(K(x_i, x_j) = \\tanh(\\gamma x_i^T x_j + r)\\)。\n",
    "\n",
    "**主要应用场景**\n",
    "\n",
    "核方法可以将线性算法扩展到非线性场景，因此应用非常广泛：\n",
    "\n",
    "1.  **支持向量机（Support Vector Machines, SVM）**: 这是核方法最经典的应用。通过引入核函数，SVM 可以有效地处理非线性分类和回归问题，寻找复杂的决策边界。\n",
    "2.  **核主成分分析（Kernel Principal Component Analysis, Kernel PCA）**: 将 PCA 通过核技巧扩展到非线性情况，用于非线性数据的降维和特征提取。\n",
    "3.  **高斯过程（Gaussian Processes, GP）**: 一种基于核函数的概率模型，常用于回归和分类，并能提供预测的不确定性估计。\n",
    "4.  **核岭回归（Kernel Ridge Regression, KRR）**: 将岭回归与核技巧结合，用于非线性回归问题。\n",
    "5.  **聚类算法**: 例如核 K-Means，可以发现非凸或非线性的簇结构。\n",
    "6.  **异常检测**: 例如单类支持向量机（One-Class SVM），利用核函数在高维空间中识别异常点。\n",
    "7.  **特征工程**: 核函数本身可以看作是一种自动化的、基于数据相似性的特征构造方式。\n",
    "\n",
    "总而言之，核方法提供了一种优雅而强大的处理非线性问题的框架，通过核函数巧妙地在高维空间中进行计算，而无需显式地进行复杂的映射。它在众多机器学习领域都有重要的应用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c453564",
   "metadata": {},
   "source": [
    "\n",
    "好的，除了核方法（Kernel Methods）之外，还有一些其他算法或技术也具有类似的效果，即将数据映射到更高维度的空间或者利用高维空间的特性来处理数据。以下是一些例子：\n",
    "\n",
    "1.  **多项式特征（Polynomial Features）**:\n",
    "    *   这是一种显式的特征工程技术。它通过计算原始特征的组合和高次幂来创建新的特征。例如，如果原始特征是 \\( (x_1, x_2) \\)，添加二次多项式特征后可能会变成 \\( (x_1, x_2, x_1^2, x_1 x_2, x_2^2) \\)，从而增加了特征维度。这使得线性模型能够学习非线性关系。\n",
    "\n",
    "2.  **随机傅里叶特征（Random Fourier Features, RFF） / Random Kitchen Sinks**:\n",
    "    *   这是一种近似核方法的技术。它不直接计算核矩阵，而是通过一个显式的随机映射函数将数据点映射到一个（通常是更高维的）欧几里得空间。在这个新的空间中，原始核函数的点积可以用新空间中的向量点积来近似。这样就可以在显式的高维空间中使用线性模型来逼近复杂的核模型，尤其适用于大规模数据集。\n",
    "\n",
    "3.  **深度学习 / 神经网络（Deep Learning / Neural Networks）**:\n",
    "    *   神经网络，特别是深度神经网络，通过其多层结构和非线性激活函数，能够学习到数据的复杂层次化表示。隐藏层可以被看作是将输入数据逐步转换到越来越抽象、可能维度更高或表达能力更强的特征空间。虽然不总是显式地“升维”到固定高维，但其学习到的表示通常蕴含了比原始输入更丰富的信息，能够捕捉复杂的非线性模式，效果上类似于在更高维空间工作。\n",
    "\n",
    "4.  **张量方法（Tensor Methods）**:\n",
    "    *   张量是多维数组，可以用来表示数据中更高阶的交互关系。一些基于张量分解（如Tucker分解、CP分解）或张量网络的机器学习方法，可以被看作是在处理和利用数据在高维张量空间中的结构，捕捉超越两两特征交互的复杂模式。\n",
    "\n",
    "5.  **特征哈希（Feature Hashing）**:\n",
    "    *   虽然其主要目的是处理大规模稀疏特征并控制内存使用，但特征哈希通过哈希函数将高维稀疏特征（可能维度极高）映射到一个维度固定但通常仍然很高的向量空间。在某些情况下，如果目标维度设置得当，也可以看作是一种将原始（可能无限维的）特征空间映射到有限高维空间的技术。\n",
    "\n",
    "这些方法各有侧重，但都提供了处理非线性问题或捕捉数据复杂结构的途径，其核心思想与核方法通过映射到高维特征空间有共通之处。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
